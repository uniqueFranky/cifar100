import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import DataLoader
import time
import os

from torch.distributed.pipelining import (
    pipeline,
    SplitPoint,
    ScheduleGPipe,
    PipelineStage,
)

from models import get_model
from utils import (
    get_dataloader,
    PerformanceMonitor,
    setup_distributed,
    cleanup_distributed,
    is_main_process
)


class LossAccumulator:
    """
    Loss ç´¯ç§¯å™¨ï¼Œç”¨äºè®°å½•æ‰€æœ‰ micro-batch çš„ lossã€‚
    è§£å†³ ScheduleGPipe ä¸ç›´æ¥è¿”å› loss çš„é—®é¢˜ã€‚
    """
    def __init__(self, criterion):
        self.criterion = criterion
        self.reset()

    def __call__(self, outputs, targets):
        """
        æ¯ä¸ª micro-batch è°ƒç”¨ä¸€æ¬¡ã€‚
        outputs: [micro_batch_size, num_classes]
        targets: [micro_batch_size]
        """
        loss = self.criterion(outputs, targets)
        batch_size = targets.size(0)

        # è®°å½•åŠ æƒ lossï¼ˆæŒ‰æ ·æœ¬æ•°ï¼‰ï¼Œé¿å…ä¸ç­‰é•¿ micro-batch æ—¶ bias
        self.total_loss += loss.detach().item() * batch_size
        self.total_samples += batch_size
        
        # åŒæ—¶è®°å½•å‡†ç¡®ç‡
        _, predicted = outputs.max(1)
        self.total_correct += predicted.eq(targets).sum().item()

        return loss

    def get_average_loss(self):
        """è¿”å›å½“å‰ç´¯è®¡çš„å¹³å‡ lossï¼Œå¹¶æ¸…ç©ºç¼“å­˜ã€‚"""
        if self.total_samples == 0:
            return 0.0

        avg = self.total_loss / self.total_samples
        return avg
    
    def get_accuracy(self):
        """è¿”å›å½“å‰ç´¯è®¡çš„å‡†ç¡®ç‡"""
        if self.total_samples == 0:
            return 0.0
        return 100. * self.total_correct / self.total_samples

    def reset(self):
        """é‡ç½®ç´¯ç§¯å™¨ã€‚"""
        self.total_loss = 0.0
        self.total_samples = 0
        self.total_correct = 0


class PipelineParallelTrainer:
    """
    æµæ°´çº¿å¹¶è¡Œè®­ç»ƒå™¨ - ä¸ DDP å®Œå…¨ç›¸åŒçš„å†å²è®°å½•æ ¼å¼
    
    å†å²è®°å½•æ ¼å¼ï¼ˆä¸ DDP ä¸€è‡´ï¼‰ï¼š
    {
        'train_loss': [],
        'train_acc': [],
        'test_loss': [],
        'test_acc': [],
        'epoch_time': [],
        'learning_rate': [],
        'gpu_memory_per_device': []
    }
    """
    def __init__(self, config):
        self.config = config
        self.world_size = config.num_gpus

    def launch(self):
        """å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒ"""

        # è®¾ç½®å¾®æ‰¹æ¬¡
        self.chunks = getattr(self.config, 'chunks', self.world_size * 4)

        # ç¡®ä¿ Batch Size èƒ½è¢« Chunks æ•´é™¤ï¼ˆä¿è¯ micro-batch ç­‰é•¿ï¼‰
        if self.config.batch_size % self.chunks != 0:
            new_bs = ((self.config.batch_size + self.chunks - 1) // self.chunks) * self.chunks
            print(f"âš ï¸  è­¦å‘Š: Batch size {self.config.batch_size} ä¸èƒ½è¢« chunks {self.chunks} æ•´é™¤")
            print(f"âœ… è‡ªåŠ¨è°ƒæ•´ Batch size ä¸º: {new_bs}")
            self.config.batch_size = new_bs

        print(f"\n{'='*60}")
        print(f"ğŸš€ å¯åŠ¨ Pipeline Parallelism è®­ç»ƒ (CIFAR-100)")
        print(f"{'='*60}")
        print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
        print(f"  - æ•°æ®é›†: CIFAR-100 (100 ç±»)")
        print(f"  - GPU æ•°é‡: {self.world_size}")
        print(f"  - GPU IDs: {self.config.gpu_ids}")
        print(f"  - Global Batch Size: {self.config.batch_size}")
        print(f"  - Micro-batches (Chunks): {self.chunks}")
        print(f"  - Micro-batch Size: {self.config.batch_size // self.chunks}")
        print(f"  - æ¨¡å‹: {self.config.model}")
        print(f"  - Epochs: {self.config.epochs}")
        print(f"  - éšæœºç§å­: {self.config.seed}")
        print(f"{'='*60}\n")
        print(f'gpu_ids: {self.config.gpu_ids}, world_size: {self.world_size}')
        mp.spawn(
            self.train_worker,
            args=(self.world_size,),
            nprocs=self.world_size,
            join=True
        )

    def _get_split_spec(self, world_size, model_name):
        """æ ¹æ® GPU æ•°é‡å’Œæ¨¡å‹ç±»å‹è¿”å›åˆ‡åˆ†ç­–ç•¥"""
        if 'resnet' in model_name.lower():
            if world_size == 2:
                return {'layer3': SplitPoint.BEGINNING}
            elif world_size == 3:
                return {
                    'layer2': SplitPoint.BEGINNING,
                    'layer3': SplitPoint.BEGINNING
                }
            elif world_size == 4:
                return {
                    'layer2': SplitPoint.BEGINNING,
                    'layer3': SplitPoint.BEGINNING,
                    'layer4': SplitPoint.BEGINNING
                }
            else:
                return {'layer2': SplitPoint.BEGINNING}
        else:
            # å…¶å®ƒæ¨¡å‹æš‚æ—¶ä¸åˆ‡
            return {}

    def train_worker(self, rank, world_size):
        """æ¯ä¸ªè¿›ç¨‹çš„è®­ç»ƒ worker"""

        # ====================================================
        # 1. åˆå§‹åŒ–åˆ†å¸ƒå¼
        # ====================================================
        gpu_id = self.config.gpu_ids[rank]
        device = torch.device(f'cuda:{gpu_id}')
        print(f"Rank {rank} ä½¿ç”¨ GPU {gpu_id} ({torch.cuda.get_device_name(gpu_id)})")
        torch.cuda.set_device(device)

        setup_distributed(rank, world_size, backend=self.config.dist_backend, device=device)

        if rank == 0:
            print(f"âœ… è¿›ç¨‹ç»„åˆå§‹åŒ–å®Œæˆ (Backend: {self.config.dist_backend})")

        # ====================================================
        # 2. ç»Ÿä¸€éšæœºç§å­
        # ====================================================
        torch.manual_seed(self.config.seed)

        if rank == 0:
            print(f"ğŸ”§ æ‰€æœ‰ rank ä½¿ç”¨ç»Ÿä¸€éšæœºç§å­: {self.config.seed}")

        # ====================================================
        # 3. æ„å»ºæ¨¡å‹å¹¶åˆ‡åˆ†ä¸ºæµæ°´çº¿ stages
        # ====================================================
        base_model = get_model(self.config.model, num_classes=self.config.num_classes)
        split_spec = self._get_split_spec(world_size, self.config.model)

        if rank == 0:
            print(f"ğŸ”§ æ„å»ºæ¨¡å‹: {self.config.model}")
            print(f"âœ‚ï¸  åˆ‡åˆ†ç­–ç•¥: {split_spec}")

        # ====================================================
        # 4. åˆ›å»ºæµæ°´çº¿ï¼ˆPipelineï¼‰
        # ====================================================
        mb_size = self.config.batch_size // self.chunks
        example_input = torch.randn(mb_size, 3, 32, 32)

        if rank == 0:
            print(f"ğŸ” ä½¿ç”¨ç¤ºä¾‹è¾“å…¥ {example_input.shape} è¿›è¡Œè¿½è¸ª...")

        pipe = pipeline(
            module=base_model,
            mb_args=(example_input,),
            split_spec=split_spec
        )

        my_submodule = pipe.get_stage_module(rank)
        my_submodule.to(device)

        stage = PipelineStage(
            my_submodule,
            stage_index=rank,
            num_stages=pipe.num_stages,
            device=device,
        )

        # ====================================================
        # 5. åˆ›å»º Loss ç´¯ç§¯å™¨å’Œè°ƒåº¦å™¨
        # ====================================================
        criterion = nn.CrossEntropyLoss()
        loss_accumulator = LossAccumulator(criterion)

        schedule = ScheduleGPipe(
            stage,
            n_microbatches=self.chunks,
            loss_fn=loss_accumulator
        )

        # ====================================================
        # 6. ä¼˜åŒ–å™¨ & å­¦ä¹ ç‡è°ƒåº¦
        # ====================================================
        optimizer = optim.SGD(
            my_submodule.parameters(),
            lr=self.config.lr,
            momentum=self.config.momentum,
            weight_decay=self.config.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä¸ DDP ä¸€è‡´ï¼‰
        if self.config.lr_schedule == 'step':
            scheduler = optim.lr_scheduler.StepLR(
                optimizer,
                step_size=self.config.lr_step_size,
                gamma=self.config.lr_gamma
            )
        elif self.config.lr_schedule == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.config.epochs
            )
        elif self.config.lr_schedule == 'multistep':
            milestones = [int(self.config.epochs * 0.5), int(self.config.epochs * 0.75)]
            scheduler = optim.lr_scheduler.MultiStepLR(
                optimizer,
                milestones=milestones,
                gamma=self.config.lr_gamma
            )
        else:
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

        # ====================================================
        # 7. æ•°æ®åŠ è½½å™¨
        # ====================================================
        trainloader, testloader, _ = get_dataloader(self.config, distributed=False)

        if rank == 0:
            print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
            print(f"ğŸ“Š è®­ç»ƒé›†: {len(trainloader.dataset)} æ ·æœ¬")
            print(f"ğŸ“Š æµ‹è¯•é›†: {len(testloader.dataset)} æ ·æœ¬\n")

        monitor = PerformanceMonitor()

        # åˆå§‹åŒ–å†å²è®°å½•ï¼ˆä¸ DDP å®Œå…¨ç›¸åŒçš„æ ¼å¼ï¼‰
        history = {}
        best_acc = 0.0
        if rank == world_size - 1:
            history = {
                'train_loss': [],
                'train_acc': [],
                'test_loss': [],
                'test_acc': [],
                'epoch_time': [],
                'learning_rate': [],
                'gpu_memory_per_device': []
            }

        dist.barrier()

        if rank == 0:
            print(f"\n{'='*60}")
            print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ {self.config.epochs} ä¸ª Epochs")
            print(f"{'='*60}\n")

        # ====================================================
        # 8. è®­ç»ƒå¾ªç¯
        # ====================================================
        for epoch in range(self.config.epochs):
            if rank == 0:
                print(f"\n{'='*60}")
                print(f"Epoch {epoch+1}/{self.config.epochs}")
                print(f"{'='*60}")

            epoch_start = time.time()
            
            # è®­ç»ƒä¸€ä¸ª epoch
            train_loss, train_acc = self.train_epoch(
                my_submodule, schedule, trainloader, optimizer,
                loss_accumulator, device, rank, world_size, epoch
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            epoch_time = time.time() - epoch_start

            # æ”¶é›†æ‰€æœ‰ rank çš„ GPU å†…å­˜ä¿¡æ¯ï¼ˆä¸ DDP æ ¼å¼ä¸€è‡´ï¼‰
            gpu_mem_allocated = torch.cuda.memory_allocated(device) / 1024**3
            gpu_mem_reserved = torch.cuda.memory_reserved(device) / 1024**3
            
            if rank == world_size - 1:
                # æ”¶é›†æ‰€æœ‰ rank çš„å†…å­˜ä¿¡æ¯
                all_gpu_ids = [torch.zeros(1, dtype=torch.int32).to(device) for _ in range(world_size)]
                all_mem_allocated = [torch.zeros(1).to(device) for _ in range(world_size)]
                all_mem_reserved = [torch.zeros(1).to(device) for _ in range(world_size)]
            else:
                all_gpu_ids = None
                all_mem_allocated = None
                all_mem_reserved = None
            
            local_gpu_id = torch.tensor([gpu_id], dtype=torch.int32).to(device)
            local_mem_allocated = torch.tensor([gpu_mem_allocated]).to(device)
            local_mem_reserved = torch.tensor([gpu_mem_reserved]).to(device)
            
            if rank == world_size - 1:
                dist.gather(local_gpu_id, gather_list=all_gpu_ids, dst=world_size-1)
                dist.gather(local_mem_allocated, gather_list=all_mem_allocated, dst=world_size-1)
                dist.gather(local_mem_reserved, gather_list=all_mem_reserved, dst=world_size-1)
            else:
                dist.gather(local_gpu_id, dst=world_size-1)
                dist.gather(local_mem_allocated, dst=world_size-1)
                dist.gather(local_mem_reserved, dst=world_size-1)
            
            # åœ¨è®­ç»ƒå¾ªç¯ä¸­
            test_loss, test_acc = self.evaluate_with_pipeline(
                my_submodule, schedule, testloader, criterion,
                loss_accumulator, device, rank, world_size
            )


            # åªåœ¨æœ€åä¸€ä¸ª rank è®°å½•å’Œæ‰“å°ï¼ˆä¸ DDP æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰
            if rank == world_size - 1:
                history['train_loss'].append(train_loss)
                history['train_acc'].append(train_acc)
                history['test_loss'].append(test_loss)
                history['test_acc'].append(test_acc)
                history['epoch_time'].append(epoch_time)
                history['learning_rate'].append(current_lr)
                
                # è®°å½•æ‰€æœ‰ GPU çš„å†…å­˜ï¼ˆä¸ DDP æ ¼å¼ä¸€è‡´ï¼‰
                gpu_mem_per_device = []
                for r in range(world_size):
                    gpu_mem_per_device.append({
                        'device_id': int(all_gpu_ids[r].item()),
                        'allocated': all_mem_allocated[r].item(),
                        'reserved': all_mem_reserved[r].item()
                    })
                history['gpu_memory_per_device'].append(gpu_mem_per_device)

                # æ‰“å°æ ¼å¼ä¸ DDP ä¸€è‡´
                print(f'\nè®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%')
                print(f'æµ‹è¯• - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%')
                print(f'å­¦ä¹ ç‡: {current_lr:.6f}, æ—¶é—´: {epoch_time:.2f}s')
                print('GPUå†…å­˜ä½¿ç”¨:')
                for mem_info in gpu_mem_per_device:
                    print(f'  GPU {mem_info["device_id"]} - '
                          f'å·²åˆ†é…: {mem_info["allocated"]:.2f}GB, '
                          f'å·²ä¿ç•™: {mem_info["reserved"]:.2f}GB')
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if test_acc > best_acc:
                    print(f'æœ€ä½³å‡†ç¡®ç‡æ›´æ–°: {best_acc:.2f}% -> {test_acc:.2f}%')
                    best_acc = test_acc
                    # self.save_checkpoint(
                    #     rank, epoch, my_submodule, optimizer, scheduler,
                    #     history, best_acc, is_best=True
                    # )
                
                # # å®šæœŸä¿å­˜
                # if (epoch + 1) % self.config.save_interval == 0:
                #     self.save_checkpoint(
                #         rank, epoch, my_submodule, optimizer, scheduler,
                #         history, best_acc
                #     )

            dist.barrier()

        # è®­ç»ƒç»“æŸ
        if rank == world_size - 1:
            print(f'\n{"="*60}')
            print('è®­ç»ƒå®Œæˆ!')
            print(f'{"="*60}')
            print(f'æ€»æ—¶é—´: {sum(history["epoch_time"]):.2f}ç§’ ({sum(history["epoch_time"])/60:.2f}åˆ†é’Ÿ)')
            print(f'å¹³å‡æ¯epoch: {sum(history["epoch_time"])/self.config.epochs:.2f}ç§’')
            print(f'æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%')

            self.save_checkpoint(
                rank,
                self.config.epochs - 1,
                my_submodule,
                optimizer,
                scheduler,
                history,
                best_acc,
                final=True
            )

        cleanup_distributed()

    def train_epoch(self, model, schedule, trainloader, optimizer,
                   loss_accumulator, device, rank, world_size, epoch):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        model.train()
        
        torch.manual_seed(self.config.seed + epoch)
        
        # é‡ç½®ç´¯ç§¯å™¨
        loss_accumulator.reset()
        
        running_loss = 0.0
        running_correct = 0
        running_total = 0
        num_batches = 0

        for batch_idx, (inputs, targets) in enumerate(trainloader):
            # æ£€æŸ¥ batch å¤§å°
            if inputs.size(0) != self.config.batch_size:
                if rank == 0:
                    print(f"âš ï¸  è·³è¿‡ä¸å®Œæ•´çš„ batch {batch_idx}")
                continue
            
            optimizer.zero_grad()

            # Stage ä¸åŒï¼Œè¡Œä¸ºä¸åŒ
            if rank == 0:
                # ç¬¬ä¸€æ®µï¼šåªéœ€è¦è¾“å…¥
                inputs = inputs.to(device, non_blocking=True)
                schedule.step(inputs)
            elif rank == world_size - 1:
                # æœ€åä¸€æ®µï¼šéœ€è¦æ ‡ç­¾ï¼Œå¹¶è®¡ç®— loss å’Œ accuracy
                targets = targets.to(device, non_blocking=True)
                schedule.step(target=targets)
                
                # è·å–è¿™ä¸ª batch çš„ç»Ÿè®¡
                loss_value = loss_accumulator.get_average_loss()
                acc_value = loss_accumulator.get_accuracy()
                
                running_loss += loss_value * loss_accumulator.total_samples
                running_correct += loss_accumulator.total_correct
                running_total += loss_accumulator.total_samples
                num_batches += 1
                
                # é‡ç½®ç´¯ç§¯å™¨ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ª batch
                loss_accumulator.reset()
            else:
                # ä¸­é—´æ®µï¼šåªåšå‰åå‘æµæ°´
                schedule.step()

            optimizer.step()

            # åªåœ¨æœ€åä¸€æ®µæ‰“å°æ—¥å¿—
            if rank == world_size - 1 and batch_idx % self.config.log_interval == 0:
                avg_loss = running_loss / running_total if running_total > 0 else 0
                avg_acc = 100. * running_correct / running_total if running_total > 0 else 0
                gpu_mem = torch.cuda.memory_allocated(device) / 1024**3
                gpu_mem_reserved = torch.cuda.memory_reserved(device) / 1024**3
                print(f'Rank {rank} | Epoch: {epoch} [{batch_idx}/{len(trainloader)}] '
                      f'Loss: {loss_value:.4f} | Acc: {acc_value:.2f}% '
                      f'| GPU Mem: {gpu_mem:.2f}GB/{gpu_mem_reserved:.2f}GB')

        # è®¡ç®—å¹³å‡ loss å’Œ accuracy
        if rank == world_size - 1:
            avg_loss = running_loss / running_total if running_total > 0 else 0
            avg_acc = 100. * running_correct / running_total if running_total > 0 else 0
            return avg_loss, avg_acc
        else:
            return 0.0, 0.0

    def evaluate_with_pipeline(self, model, schedule, testloader, criterion, 
                            loss_accumulator, device, rank, world_size):
        """
        ä½¿ç”¨ Pipeline è¿›è¡Œè¯„ä¼°
        """
        model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        loss_accumulator.reset()
        
        running_loss = 0.0
        running_correct = 0
        running_total = 0
        num_batches = 0
        
        # å®Œå…¨ç§»é™¤torch.no_grad()ï¼Œè®©Pipelineæ­£å¸¸å·¥ä½œ
        for batch_idx, (inputs, targets) in enumerate(testloader):
            # æ£€æŸ¥ batch å¤§å°ï¼Œç¡®ä¿æ‰€æœ‰rankå¤„ç†ç›¸åŒæ•°é‡çš„batch
            if inputs.size(0) != self.config.batch_size:
                continue
            
            # Stage ä¸åŒï¼Œè¡Œä¸ºä¸åŒ
            if rank == 0:
                inputs = inputs.to(device, non_blocking=True)
                schedule.step(inputs)
            elif rank == world_size - 1:
                targets = targets.to(device, non_blocking=True)
                schedule.step(target=targets)
                
                loss_value = loss_accumulator.get_average_loss()
                acc_value = loss_accumulator.get_accuracy()
                
                running_loss += loss_value * loss_accumulator.total_samples
                running_correct += loss_accumulator.total_correct
                running_total += loss_accumulator.total_samples
                num_batches += 1
                
                loss_accumulator.reset()
            else:
                # ä¸­é—´stageä¹Ÿéœ€è¦å‚ä¸Pipeline
                schedule.step()
        
        # è®¡ç®—å¹³å‡å€¼
        if rank == world_size - 1:
            avg_loss = running_loss / running_total if running_total > 0 else 0
            avg_acc = 100. * running_correct / running_total if running_total > 0 else 0
            
            # å¹¿æ’­ç»“æœ
            result = torch.tensor([avg_loss, avg_acc]).to(device)
            dist.broadcast(result, src=world_size-1)
            
            return avg_loss, avg_acc
        else:
            result = torch.zeros(2).to(device)
            dist.broadcast(result, src=world_size-1)
            
            return result[0].item(), result[1].item()



    def save_checkpoint(self, rank, epoch, model, optimizer, scheduler,
                       history, best_acc, is_best=False, final=False):
        """ä¿å­˜ Checkpointï¼ˆä¸ DDP æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰"""
        os.makedirs(self.config.save_dir, exist_ok=True)
        print(f"Rank {rank} æ­£åœ¨ä¿å­˜ checkpoint..., is_best={is_best}, final={final}")
        # è½¬æ¢ config ä¸ºå­—å…¸ï¼ˆä¸ DDP ä¸€è‡´ï¼‰
        config_dict = self.config.__dict__.copy() if hasattr(self.config, '__dict__') else self.config


        if final:
            path = self.config.final_checkpoint_path
        if path is None:
            return

        # ä¿å­˜æ ¼å¼ï¼ˆä¸ DDP ä¸€è‡´ï¼‰
        state = {
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_acc': best_acc,
            'config': config_dict,
            # Pipeline ç‰¹æœ‰çš„é¢å¤–ä¿¡æ¯
            'stage_index': rank,
            'num_stages': self.world_size,
            'history': history
        }


        torch.save(state, path)
        
        if is_best:
            print(f'ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {path}')
        elif final:
            print(f'ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {path}')
        else:
            print(f'ä¿å­˜checkpointåˆ°: {path}')
